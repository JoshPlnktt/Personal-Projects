"""
This module scrapes the data from https://theresanaiforthat.com/, returning a csv in the working directory with all the data.
"""

import logging
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import pandas as pd
from scrapingbee import ScrapingBeeClient

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')


def get_html(base_url, retries=5):
    """
    This function scrapes the html from the home page for further use.

    Arguments:
    - base_url: The base url (https://theresanaiforthat.com/)
    - retries: Number of retries if a proxy fails

    Returns:
    - soup: A BeautifulSoup object with the HTML of the home page
    """
    logging.info("Getting page HTML...")
    client = ScrapingBeeClient(
        api_key='')  # Enter your API key here

    for attempt in range(retries):
        try:
            response = client.get(base_url, timeout=10)
            response.raise_for_status()
            logging.info("Home page HTML status: %s", response.status_code)
            soup = BeautifulSoup(response.content, "html.parser")
            return soup
        except requests.exceptions.RequestException as e:
            logging.error("Attempt %d failed: %s", attempt + 1, e)
            if attempt < retries - 1:
                logging.info("Retrying with a different proxy...")
            else:
                logging.error("Max retries exceeded. Exiting...")
                return None


def get_links(soup):
    """
    This function scrapes individual product links from the home page HTML.

    Arguments:
    - soup: the html object of the home page created by get_html()
    """
    url = "https://theresanaiforthat.com"
    links = []
    for link in soup.find_all("a", class_="stats"):
        extension = link.get("href")
        full_link = f"{url}{extension}"
        links.append(full_link)
    logging.info("%d different products found.", len(links))
    return links


def get_description(soup):
    """
    This function extracts the description from the HTML.

    Arguments:
    - soup: the html object of the page
    """
    description = " ".join([p.text.strip()
                           for p in soup.select("div.description p")])
    return description


def get_tags(soup):
    """
    This function extracts the tags from the HTML.

    Arguments:
    - soup: the html object of the page

    Returns:
    - tags: A string containing the tags separated by commas
    """
    tags = [tag.text.strip().lower() for tag in soup.select("div.tags a.tag")]
    return ", ".join(tags)


def get_price(soup):
    """
    This function extracts the price from the HTML.

    Arguments:
    - soup: the html object of the page

    Returns:
    - price: A string containing the price
    """
    price_tag = soup.select_one("div.tags span.tag.price")
    return price_tag.text.strip() if price_tag else "No price available"


def get_data(links):
    """
    This function takes in the links one at a time, scrapes and parses it, and returns a dataframe.

    Arguments:
    - links: A list of links generated by get_links()
    """
    logging.info("Scraping product info now...")
    product_counter = 0

    titles = []
    use_cases = []
    websites = []
    descriptions = []
    logos = []
    authors = []
    avg_ratings = []
    rating_counts = []
    primary_tasks = []
    tags = []
    prices = []
    columns = [titles, use_cases, descriptions, websites, logos, authors,
               avg_ratings, rating_counts, primary_tasks, tags, prices]

    for link in links:
        logging.info("Processing link: %s", link)
        page_html = get_html(link)
        if page_html is None:
            logging.error("Skipping link due to error: %s", link)
            continue
        title = page_html.select_one("h1.title_inner").text.strip()
        use_case = page_html.find("div", id="use_case").get_text()
        website = page_html.select_one(
            "a.ai_top_link.visit_website_btn").get("href")
        description = get_description(page_html)
        logo = page_html.select_one("img.taaft_icon").get("src")
        author_tag = page_html.select_one(
            "div.attribution_wrap .user_name")
        author = author_tag.text.strip() if author_tag else "Unknown Author"
        rating_tag = page_html.select_one("div.average_rating_score")
        rating = float(rating_tag.text.strip()) if rating_tag else 0
        rating_count_tag = page_html.select_one(
            "div.ratings_chart_average_from")
        rating_count_text = rating_count_tag.text.strip().split()[2]
        rating_count_text = rating_count_tag.text.strip().split()[2]
        if rating_count_text.lower() == "yet.":
            rating_count = 0
        else:
            try:
                rating_count = int(rating_count_text)
            except ValueError:
                rating_count = 0
        primary_task = page_html.select_one(
            "span.task_label").get_text(strip=True)
        tag = get_tags(page_html)
        price = get_price(page_html)

        cells = [title, use_case, description, website, logo, author,
                 rating, rating_count, primary_task, tag, price]

        for column, cell in zip(columns, cells):
            column.append(cell)

        product_counter += 1
        logging.info("Product %d completed.", product_counter)

    logging.info("All data retrieved. Creating a dataframe...")
    data = pd.DataFrame({
        'Title': titles,
        "Use Cases": use_cases,
        'Description': descriptions,
        'Website': websites,
        'Logo': logos,
        'Author': authors,
        'Average Rating': avg_ratings,
        'Rating Count': rating_counts,
        'Primary Task': primary_tasks,
        'Tags': tags,
        'Price': prices
    })

    return data


def get_csv(df, csv_name):
    """
    This function saves the dataframe as a csv.
    It uses the current date-time as part of the name.

    Arguments:
    - df: A dataframe to save to csv
    - csv_name: the name for the csv, will come before the date.
        - ex. csv_name_{current_date}.csv
    """
    current_date = datetime.now().strftime('%Y-%m-%d')
    file_name = f'{csv_name}_{current_date}.csv'
    df.to_csv(file_name, index=False)
    logging.info("CSV file saved as: %s", file_name)


def main(site_url):
    """
    A function which combines all other helper functions to navigate from page to page,
    and scrape data from https://theresanaiforthat.com/.

    Arguments:
    - url: The base url showing a list of all ai (https://theresanaiforthat.com/)
    """
    # Get home page html
    home_html = get_html(site_url)
    if home_html is None:
        logging.error("Failed to retrieve home page HTML. Exiting...")
        return

    # Parse links to individual pages
    links = get_links(home_html)

    # Retrieve information from each page, make df
    data = get_data(links)

    # Save data as csv in working directory
    get_csv(data, "ai_list")


if __name__ == "__main__":
    TARGET = "https://theresanaiforthat.com/"
    main(site_url=TARGET)
